---
title: "Project II Analysis"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setting up the enviornment - Install the packages
```{r, results="hide", message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(multcomp)
library(knitr)
library(MASS)
library(tools)
import::from(multcomp, glht, mcp, contrMat)
import::from(broom, tidy)
source("util.R") 
```

# Data wrangling with output file generated by software
```{r, message=FALSE, results='asis', warning=FALSE}
map_condition <- read_csv("mapcondition.csv")

# read all experiment .csv files
files = list.files(path="./experiment-data", pattern="*.csv", full.names = TRUE)

# this line can be used to read each file individually
# for (i in 1:length(files)) assign(file_path_sans_ext(files[i]), read.csv(file.path("experiment-data", files[i])))

# combine all experiment data into one table
data <- files %>% 
  lapply(read_csv) %>% 
  bind_rows 

# add IV columns
data[, "EO"] <- NA
data[, "TL"] <- NA
data[, "ES"] <- NA
data[, "SS"] <- NA

# map endIndex to condition
for(i in 1:nrow(data)){
  # starting index of table is one
  index_that_maps <- data$endIndex[i] + 1
  row_to_map <- map_condition[index_that_maps,]
  data[i,]$EO <- row_to_map$`Edge orientation`
  data[i,]$TL <- row_to_map$`Target location`
  data[i,]$ES <- row_to_map$`Edge section`
  data[i,]$SS <- row_to_map$`Side section`
}

# calculating error rates for each participant
error_rate_list <- unique(data$participantId)
error_rate_tb <- data.frame(Reduce(rbind, error_rate_list))
colnames(error_rate_tb)[1] <- "Participant_ID"
error_rate_tb[, "error rate"] <- NA


for (i in 1:nrow(error_rate_tb)){
  # total trials is 288
  error_rate <- length(data[data$participantId == i & data$error == "true", c('error')]$error) / 288
  error_rate_tb[i,]$'error rate' <- error_rate
}

# Calculate error rate in total (total trials 2304)
amount_errors <- length(data[data$error == "true", c('error')]$error) / 2304

MT <- aggregate(data$MT, list("ID"= data$participantId), mean)
colnames(MT) <- c("Participant_ID","MT")

data_h2 <- merge(error_rate_tb,MT,by="Participant_ID")


kable(head(data, n=5), caption="A look at some rows in the experiment data", align = c("l"))

kable(error_rate_tb, caption="The error rate of each participant", align = c("l"))

```

# Visualizing the data

# Addressing our first hypothesis
##### H1: It is faster to move to targets on the top edge of the screen rather than the bottom edge.
```{r, message=FALSE}
# to address the first hypothesis, we group by Edge Orientation (Horizontal) and Edge Section (Top and Bottom)
data_horizontal_edge <- data[data$EO == "Horizontal",]

# Boxplot
data_horizontal_edge %>%
  ggplot(aes(x = ES, y = MT)) +
  geom_boxplot() +
  labs(title = "Box plots of edge section on movement time", x = "Edge Section", y = "Movement Time")

# Violin plot  
h1_violin_plot <-
  data_horizontal_edge %>%
  ggplot(aes(x = ES, y = MT)) +
  geom_violin() +
  geom_point(color = "lightgray") +
  labs(title = "Violin plots of edge section on movement time", x = "Edge Section", y = "Movement Time")

h1_violin_plot + stat_summary(fun.y=mean, geom="point", size=2, color="red")

# check t-test assumptions

# Shapiro-Wilk normality test for top edge movement time
top_edge_movement_time <- data_horizontal_edge[data_horizontal_edge$ES == "Top", c('MT')]$MT
shapiro.test(top_edge_movement_time)
shapiro.test(log(top_edge_movement_time))

# Shapiro-Wilk normality test for bottom edge movement time
bottom_edge_movement_time <- data_horizontal_edge[data_horizontal_edge$ES == "Bottom", c('MT')]$MT
shapiro.test(bottom_edge_movement_time)
shapiro.test(log(bottom_edge_movement_time))

# Since the data does not seem to be distributed normally due to the p-values above, even with a log transform, we will use the non parametric two-samples Wilcoxon rank test.
wilcox.test(top_edge_movement_time, bottom_edge_movement_time, alternative = "two.sided")

# With a p-value of 2.562e-15, we can conclude that the mean movement time for the top edge is significantly different than the mean movement time for the bottom edge. This means that it appears that the mean movement time to the top edge is faster than mean movement time to the bottom edge. This confirms our hypothesis.
```

# Addressing our second hypothesis
##### H2: For left-handed participants it is faster to move to targets on the right edge of the screen rather than the left edge, whereas for the right-handed participants it is faster to move to targets at the left edge of the screen rather than the right edge.

We could not test our second hypothesis due to not having the sufficient data collected (having left-handed participants). We however, will do some exploratory data analysis instead.

#Visualize the data for interaction between Error rates and MT with by Participant ID
```{r}
data_vertical_edge <- data[data$EO == "Vertical",]

p_data <- 
  data_h2 %>% 
  ggplot(aes(x = `error rate`, y = MT, color = Participant_ID)) +
  stat_summary(fun.y=mean, geom="point", size=2, color="red") +
  geom_line()
p_data

```
#Code for Learning effect
```{r}
learning_dominant <- data_vertical_edge %>% filter(SS == "Dominant")
learning_non_dominant <- data_vertical_edge %>% filter(SS == "Non-Dominant")
ggplot(learning_dominant, aes(x=participantId, y=MT, group=participantId)) + geom_boxplot() + ggtitle("Dominant Learning")
ggplot(learning_non_dominant, aes(x=participantId, y=MT, group=participantId)) + geom_boxplot() + ggtitle("Non-Dominant Learning")


```

#Code for Interaction effect between Error and Side Section
```{r}
#Asses the above linear model with goodness of fit ANOVA
m_ss_er <- lm(MT ~ SS * error, data = data)
anova(m_ss_er)

#As the interaction effect is significant
# 1. construct the matrix for each variable
ss_mat <- contrMat(table(data$SS), "Tukey")

# 2. pad other columns with zero
t_rows <- cbind(ss_mat,       (ss_mat * 0))
b_rows <- cbind((ss_mat * 0),   ss_mat)

# 3. add hypothesis labels as row names
rownames(t_rows) <- paste(levels(data$ES)[1],  rownames(t_rows), sep =": ")
rownames(b_rows)  <- paste(levels(data$ES)[2],  rownames(b_rows),  sep =": ")

# 4. concatenate them together
contrast_matrix <- rbind(
  t_rows,
  b_rows
)
contrast_matrix

# We will also need a model *only* with the interaction term
m_interaction <- update(m_ss_er, .~. - SS - error -1)

# use the contrast matrix in glht() in place of "mcp()"
pairwise_interaction <- glht(m_interaction, linfct = contrast_matrix)  
p_pairwise_interaction <- 
  tidy(confint(pairwise_interaction)) %>% 
  plot_glht()
p_pairwise_interaction
```

